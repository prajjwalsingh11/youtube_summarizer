import os
import nltk
from nltk.data import find
import streamlit as st
from youtube_transcript_api import YouTubeTranscriptApi
from langchain.text_splitter import RecursiveCharacterTextSplitter
from dotenv import load_dotenv
from fpdf import FPDF
import yt_dlp
from docx import Document
from io import BytesIO
import re
import math
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from pytube import YouTube
from pytube.exceptions import PytubeError
from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip
import cv2
import subprocess
from urllib.error import HTTPError
from openai import OpenAI

# Check if the vader_lexicon is already downloaded 
try: 
    find('sentiment/vader_lexicon.zip') 
except LookupError: 
    nltk.download('vader_lexicon')

def load_environment():
    """Load environment variables"""
    env_path = r"C:\coding\youtube summarizer\youtube_summarizer\project.env"
    if os.path.exists(env_path):
        load_dotenv(env_path)
    
    api_key = os.getenv('GROQ_API_KEY')
    if not api_key:
        raise ValueError("GROQ_API_KEY not found in environment variables")
    
    return api_key

# Initialize Groq client
try:
    api_key = load_environment()
    groq_client = OpenAI(
        api_key=api_key,
        base_url="https://api.groq.com/openai/v1"
    )
except Exception as e:
    st.error(f"Error initializing API client: {str(e)}")
    st.stop()

def extract_video_id(youtube_url):
    """Extract video ID from different YouTube URL formats"""
    patterns = [
        r'(?:v=|\/)([0-9A-Za-z_-]{11}).*',  # Standard and shared URLs
        r'(?:embed\/)([0-9A-Za-z_-]{11})',   # Embed URLs
        r'(?:youtu\.be\/)([0-9A-Za-z_-]{11})',  # Shortened URLs
        r'(?:shorts\/)([0-9A-Za-z_-]{11})',   # YouTube Shorts
        r'^([0-9A-Za-z_-]{11})$'  # Just the video ID
    ]
    
    youtube_url = youtube_url.strip()
    
    for pattern in patterns:
        match = re.search(pattern, youtube_url)
        if match:
            return match.group(1)
    
    raise ValueError("Could not extract video ID from URL")

def get_transcript(youtube_url):
    """Get transcript using YouTube Transcript API with cookies"""
    try:
        video_id = extract_video_id(youtube_url)
        
        # Get cookies file path
        cookies_file = os.getenv('COOKIE_PATH', os.path.join(os.path.dirname(__file__), 'cookies.txt'))
        
        if not os.path.exists(cookies_file):
            st.error("Cookie file not found. Please follow the setup instructions in the README.")
            return None, None
            
        try:
            # Read cookies from file
            with open(cookies_file, 'r') as f:
                cookies_content = f.read()
                if not cookies_content.strip():
                    st.error("Cookie file is empty. Please re-export your YouTube cookies.")
                    return None, None
            
            # Get transcript with cookies
            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id, cookies=cookies_file)
            
            try:
                transcript = transcript_list.find_manually_created_transcript()
            except:
                try:
                    transcript = next(iter(transcript_list))
                except Exception as e:
                    st.error("Your YouTube cookies might have expired. Please re-export your cookies and try again.")
                    return None, None
            
            full_transcript = " ".join([part['text'] for part in transcript.fetch()])
            language_code = transcript.language_code
            
            return full_transcript, language_code
                
        except Exception as e:
            st.error("Authentication failed. Please update your cookies.txt file with fresh YouTube cookies.")
            st.info("Tip: Sign in to YouTube again and re-export your cookies using the browser extension.")
            return None, None
            
    except Exception as e:
        st.error("Invalid YouTube URL. Please check the link and try again.")
        return None, None


def get_available_languages():
    """Return a dictionary of available languages"""
    return {
        'English': 'en',
        'Hindi' : 'hi',
        'Deutsch': 'de',
        'Italiano': 'it',
        'EspaÃ±ol': 'es',
        'FranÃ§ais': 'fr',
        'Nederlands': 'nl',
        'Polski': 'pl',
        'æ—¥æœ¬èª': 'ja',
        'ä¸­æ–‡': 'zh',
        'Ğ ÑƒÑÑĞºĞ¸Ğ¹': 'ru',
        'í•œêµ­ì–´': 'ko',  # Korean
        'PortuguÃªs': 'pt',  # Portuguese
        'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©': 'ar',  # Arabic
        'TÃ¼rkÃ§e': 'tr',  # Turkish
        'à¦¬à¦¾à¦‚à¦²à¦¾': 'bn',  # Bengali
        'à¤®à¤°à¤¾à¤ à¥€': 'mr',  # Marathi
        'à®¤à®®à®¿à®´à¯': 'ta',  # Tamil
        'à°¤à±†à°²à±à°—à±': 'te',  # Telugu
        'à²•à²¨à³à²¨à²¡': 'kn',  # Kannada
        'à´®à´²à´¯à´¾à´³à´‚': 'ml',  # Malayalam
        'à¤­à¥‹à¤œà¤ªà¥à¤°à¥€': 'bh' # Bhojpuri
    }

# Function to create summary prompt
def create_summary_prompt(text, target_language, mode='video'):
    """Create an optimized prompt for summarization in the target language and mode"""
    language_prompts = {
        'en': {
            'title': 'TITLE',
            'overview': 'OVERVIEW',
            'key_points': 'KEY POINTS',
            'takeaways': 'MAIN TAKEAWAYS',
            'context': 'CONTEXT & IMPLICATIONS'
        },
        'hi': {
            'title': 'à¤¶à¥€à¤°à¥à¤·à¤•',
            'overview': 'à¤…à¤µà¤²à¥‹à¤•à¤¨',
            'key_points': 'à¤®à¥à¤–à¥à¤¯ à¤¬à¤¿à¤‚à¤¦à¥',
            'takeaways': 'à¤®à¥à¤–à¥à¤¯ à¤¨à¤¿à¤·à¥à¤•à¤°à¥à¤·',
            'context': 'à¤ªà¥à¤°à¤¸à¤‚à¤— à¤”à¤° à¤ªà¥à¤°à¤­à¤¾à¤µ'
        },
        'de': {
            'title': 'TITEL',
            'overview': 'ÃœBERBLICK',
            'key_points': 'KERNPUNKTE',
            'takeaways': 'HAUPTERKENNTNISSE',
            'context': 'KONTEXT & AUSWIRKUNGEN'
        },
        'it': { 
            'title': 'TITOLO',
            'overview': 'PANORAMICA',
            'key_points': 'PUNTI CHIAVE',
            'takeaways': 'CONCLUSIONI PRINCIPALI',
            'context': 'CONTESTO E IMPLICAZIONI'
        },
        'es': {
            'title': 'TÃTULO',
            'overview': 'VISIÃ“N GENERAL',
            'key_points': 'PUNTOS CLAVE',
            'takeaways': 'CONCLUSIONES PRINCIPALES',
            'context': 'CONTEXTO E IMPLICACIONES'
        },
        'fr': {
            'title': 'TITRE',
            'overview': 'APERÃ‡U',
            'key_points': 'POINTS CLÃ‰S',
            'takeaways': 'CONCLUSIONS PRINCIPALES',
            'context': 'CONTEXT & IMPLICATIONS'
        },
        'nl': {
            'title': 'TITEL',
            'overview': 'OVERZICHT',
            'key_points': 'BELANGRIJKE PUNTEN',
            'takeaways': 'HOOFDRESULTATEN',
            'context': 'CONTEXT & IMPLIKATIES'
        },
        'pl': {
            'title': 'TYTUÅ',
            'overview': 'PRZEGLÄ„D',
            'key_points': 'KLUCZOWE PUNKTY',
            'takeaways': 'GÅÃ“Ğ’ĞĞ«Ğ• Ğ’Ğ«Ğ’ĞĞ”Ğ«',
            'context': 'ĞšĞĞĞ¢Ğ•ĞšĞ¡Ğ¢ Ğ˜ Ğ˜ĞœĞŸĞ›Ğ˜ĞšĞĞ¦Ğ˜Ğ˜'
        },
        'ja': {
            'title': 'ã‚¿ã‚¤ãƒˆãƒ«',
            'overview': 'æ¦‚è¦',
            'key_points': 'ä¸»ãªãƒã‚¤ãƒ³ãƒˆ',
            'takeaways': 'ä¸»ãªçµè«–',
            'context': 'æ–‡è„ˆã¨å½±éŸ¿'
        },
        'zh': {
            'title': 'æ ‡é¢˜',
            'overview': 'æ¦‚è¿°',
            'key_points': 'å…³é”®ç‚¹',
            'takeaways': 'ä¸»è¦ç»“è®º',
            'context': 'èƒŒæ™¯ä¸æ„ä¹‰'
        },
        'ru': {
            'title': 'Ğ—ĞĞ“ĞĞ›ĞĞ’ĞĞš',
            'overview': 'ĞĞ‘Ğ—ĞĞ ',
            'key_points': 'ĞšĞ›Ğ®Ğ§Ğ•Ğ’Ğ«Ğ• ĞŸĞ£ĞĞšĞ¢Ğ«',
            'takeaways': 'ĞĞ¡ĞĞĞ’ĞĞ«Ğ• Ğ’Ğ«Ğ’ĞĞ”Ğ«',
            'context': 'ĞšĞĞĞ¢Ğ•ĞšĞ¡Ğ¢ Ğ˜ Ğ—ĞĞĞ§Ğ•ĞĞ˜Ğ•'
        },
        'ko': {
            'title': 'ì œëª©',
            'overview': 'ê°œìš”',
            'key_points': 'í•µì‹¬ í¬ì¸íŠ¸',
            'takeaways': 'ì£¼ìš” ê²°ë¡ ',
            'context': 'ë§¥ë½ ë° ì˜ë¯¸'
        },
        'pt': {
            'title': 'TÃTULO',
            'overview': 'VISÃƒO GERAL',
            'key_points': 'PONTOS PRINCIPAIS',
            'takeaways': 'CONCLUSÃ•ES PRINCIPAIS',
            'context': 'CONTEXTO E IMPLICAÃ‡Ã•ES'
        },
        'ar': {
            'title': 'Ø§Ù„Ø¹Ù†ÙˆØ§Ù†',
            'overview': 'Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø©',
            'key_points': 'Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©',
            'takeaways': 'Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©',
            'context': 'Ø§Ù„Ø³ÙŠØ§Ù‚ ÙˆØ§Ù„Ø¢Ø«Ø§Ø±'
        },
        'tr': {
            'title': 'BAÅLIK',
            'overview': 'GENEL BAKIÅ',
            'key_points': 'TEMEL NOKTALAR',
            'takeaways': 'ANA SONUÃ‡LAR',
            'context': 'BAÄLAM VE ETKÄ°LER'
        },
        'bn': {
            'title': 'à¦¶à¦¿à¦°à§‹à¦¨à¦¾à¦®',
            'overview': 'à¦“à¦­à¦¾à¦°à¦­à¦¿à¦‰',
            'key_points': 'à¦®à§‚à¦² à¦¬à¦¿à¦·à¦¯à¦¼',
            'takeaways': 'à¦®à§à¦–à§à¦¯ à¦¬à¦¿à¦·à¦¯à¦¼à¦—à§à¦²à¦¿',
            'context': 'à¦ªà§à¦°à¦¸à¦™à§à¦— à¦“ à¦ªà§à¦°à¦­à¦¾à¦¬'
        },
        'mr': {
            'title': 'à¤¶à¥€à¤°à¥à¤·à¤•',
            'overview': 'à¤†à¤¢à¤¾à¤µà¤¾',
            'key_points': 'à¤®à¤¹à¤¤à¥à¤¤à¥à¤µà¤¾à¤šà¥‡ à¤®à¥à¤¦à¥à¤¦à¥‡',
            'takeaways': 'à¤®à¥à¤–à¥à¤¯ à¤¨à¤¿à¤·à¥à¤•à¤°à¥à¤·',
            'context': 'à¤¸à¤‚à¤¦à¤°à¥à¤­ à¤†à¤£à¤¿ à¤ªà¤°à¤¿à¤£à¤¾à¤®'
        },
        'ta': {
            'title': 'à®¤à®²à¯ˆà®ªà¯à®ªà¯',
            'overview': 'à®…à®µà®²à¯‹à®•à®©à®®à¯',
            'key_points': 'à®®à¯à®•à¯à®•à®¿à®¯ à®…à®®à¯à®šà®™à¯à®•à®³à¯',
            'takeaways': 'à®®à¯à®•à¯à®•à®¿à®¯ à®®à¯à®Ÿà®¿à®µà¯à®•à®³à¯',
            'context': 'à®šà¯‚à®´à®²à¯ à®®à®±à¯à®±à¯à®®à¯ à®µà®¿à®³à¯ˆà®µà¯à®•à®³à¯'
        },
        'te': {
            'title': 'à°¶à±€à°°à±à°·à°¿à°•',
            'overview': 'à°¸à°®à°—à±à°°à°‚',
            'key_points': 'à°®à±à°–à±à°¯ à°…à°‚à°¶à°¾à°²à±',
            'takeaways': 'à°ªà±à°°à°§à°¾à°¨ à°ªà°¾à°¯à°¿à°‚à°Ÿà±à°²à±',
            'context': 'à°¸à°‚à°¦à°°à±à°­à°‚ à°®à°°à°¿à°¯à± à°ªà±à°°à°¤à°¿à°«à°²à°¾à°²à±'
        },
        'kn': {
            'title': 'à²¶à³€à²°à³à²·à²¿à²•à³†',
            'overview': 'à²†à²µà²²à³‹à²•à²¨',
            'key_points': 'à²®à³à²–à³à²¯ à²…à²‚à²¶à²—à²³à³',
            'takeaways': 'à²ªà³à²°à²®à³à²– à²¤à²¤à³à²µà²—à²³à³',
            'context': 'à²¸à²‚à²¦à²°à³à²­ à²®à²¤à³à²¤à³ à²ªà²°à²¿à²£à²¾à²®à²—à²³à³'
        },
        'ml': {
            'title': 'à´¤à´²à´•àµà´•àµ†à´Ÿàµà´Ÿàµ',
            'overview': 'à´…à´µà´²àµ‹à´•à´¨à´‚',
            'key_points': 'à´ªàµà´°à´§à´¾à´¨ à´ªàµ‹à´¯à´¿à´¨àµà´±àµà´•àµ¾',
            'takeaways': 'à´®àµà´–àµà´¯ à´¸à´®à´¾à´¹à´¾à´°à´™àµà´™àµ¾',
            'context': 'à´¸à´¨àµà´¦àµ¼à´­à´µàµà´‚ à´ªàµà´°à´¤à´¿à´«à´²à´™àµà´™à´³àµà´‚'
        },
        'bh': {
            'title': 'à¤¶à¥€à¤°à¥à¤·à¤•',
            'overview': 'à¤“à¤µà¤°à¤µà¥à¤¯à¥‚',
            'key_points': 'à¤®à¥à¤–à¥à¤¯ à¤¬à¤¿à¤‚à¤¦à¥',
            'takeaways': 'à¤®à¥à¤–à¥à¤¯ à¤¨à¤¿à¤·à¥à¤•à¤°à¥à¤·',
            'context': 'à¤ªà¥à¤°à¤¸à¤‚à¤— à¤”à¤° à¤ªà¥à¤°à¤­à¤¾à¤µ'
        }
    }

    prompts = language_prompts.get(target_language, language_prompts['en'])

    if mode == 'podcast':
        system_prompt = f"""You are an expert content analyst and summarizer. Create a comprehensive 
        podcast-style summary in {target_language}. Ensure all content is fully translated and culturally adapted 
        to the target language."""

        user_prompt = f"""Please provide a detailed podcast-style summary of the following content in {target_language}. 
        Structure your response as follows, making it an interactive dialogue between two individuals:

        ğŸ™ï¸ {prompts['title']}: Create an engaging title

        ğŸ—£ï¸ Interactive Dialogue: 
        - Begin with a question from one person and a response from another.
        - Continue with a back-and-forth conversation, exploring key arguments and points.
        - Include examples and anecdotes as part of the dialogue.
        - Ensure the conversation flows naturally and engagingly.

        ğŸ“ˆ {prompts['takeaways']}:
        - List 5-7 practical insights discussed during the conversation.
        - Explain their significance and potential impact.

        Text to summarize: {text}

        Ensure the summary is comprehensive enough for someone who hasn't seen the original content."""

    else:
        system_prompt = f"""You are an expert content analyst and summarizer. Create a comprehensive 
        video-style summary in {target_language}. Ensure all content is fully translated and culturally adapted 
        to the target language."""

        user_prompt = f"""Please provide a detailed video-style summary of the following content in {target_language}. 
        Structure your response as follows:

        ğŸ¯ {prompts['title']}: Create a descriptive title

        ğŸ“ {prompts['overview']} (2-3 sentences):
        - Provide a brief context and main purpose

        ğŸ”‘ {prompts['key_points']}:
        - Extract and explain the main arguments
        - Include specific examples
        - Highlight unique perspectives

        ğŸ’¡ {prompts['takeaways']}:
        - List 3-5 practical insights
        - Explain their significance

        ğŸ”„ {prompts['context']}:
        - Broader context discussion
        - Future implications

        Text to summarize: {text}

        Ensure the summary is comprehensive enough for someone who hasn't seen the original content."""

    return system_prompt, user_prompt


# Function to create prompts
def create_prompts(language_code, section_number, text_chunk, mode):
    language_instructions = {
        'en': 'Create a detailed summary of the following section in English. Maintain all important information, arguments, and connections.',
        'hi': 'à¤•à¥ƒà¤ªà¤¯à¤¾ à¤¨à¤¿à¤®à¥à¤¨à¤²à¤¿à¤–à¤¿à¤¤ à¤–à¤‚à¤¡ à¤•à¤¾ à¤¹à¤¿à¤‚à¤¦à¥€ à¤®à¥‡à¤‚ à¤¸à¤‚à¤•à¥à¤·à¤¿à¤ªà¥à¤¤ à¤µà¤°à¥à¤£à¤¨ à¤•à¤°à¥‡à¤‚à¥¤ à¤¸à¤­à¥€ à¤®à¤¹à¤¤à¥à¤µà¤ªà¥‚à¤°à¥à¤£ à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€, à¤¤à¤°à¥à¤• à¤”à¤° à¤•à¤¨à¥‡à¤•à¥à¤¶à¤¨ à¤¬à¤¨à¤¾à¤ à¤°à¤–à¥‡à¤‚à¥¤',
        'de': 'Erstellen Sie eine detaillierte Zusammenfassung des folgenden Abschnitts auf Deutsch. Behalten Sie alle wichtigen Informationen, Argumente und Verbindungen bei.',
        'it': 'Crea un riassunto dettagliato della seguente sezione in italiano. Mantieni tutte le informazioni importanti, gli argomenti e le connessioni.',
        'es': 'Cree un resumen detallado de la siguiente secciÃ³n en espaÃ±ol. Mantenga toda la informaciÃ³n importante, los argumentos y las conexiones.',
        'fr': 'CrÃ©ez un rÃ©sumÃ© dÃ©taillÃ© de la section suivante en franÃ§ais. Conservez toutes les informations importantes, arguments et connexions.',
        'nl': 'Maak een gedetailleerde samenvatting van het volgende gedeelte in het Nederlands. Behoud alle belangrijke informatie, argumenten en verbindingen.',
        'pl': 'UtwÃ³rz szczegÃ³Å‚owe podsumowanie nastÄ™pujÄ…cej sekcji po polsku. Zachowaj wszystkie waÅ¼ne informacje, argumenty i poÅ‚Ä…czenia.',
        'ja': 'ä»¥ä¸‹ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®è©³ç´°ãªè¦ç´„ã‚’æ—¥æœ¬èªã§ä½œæˆã—ã¦ãã ã•ã„ã€‚ã™ã¹ã¦ã®é‡è¦ãªæƒ…å ±ã€è­°è«–ã€ãŠã‚ˆã³æ¥ç¶šã‚’ç¶­æŒã—ã¾ã™ã€‚',
        'zh': 'ç”¨ä¸­æ–‡åˆ›å»ºä»¥ä¸‹éƒ¨åˆ†çš„è¯¦ç»†æ‘˜è¦ã€‚ä¿ç•™æ‰€æœ‰é‡è¦ä¿¡æ¯ã€è®ºç‚¹å’Œè¿æ¥ã€‚',
        'ru': 'Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ¹Ñ‚Ğµ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ğ¾Ğµ Ñ€ĞµĞ·ÑĞ¼Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ° Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚Ğµ Ğ²ÑÑ Ğ²Ğ°Ğ¶Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, Ğ°Ñ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ ÑĞ²ÑĞ·Ğ¸.',
        'ko': 'ë‹¤ìŒ ì„¹ì…˜ì— ëŒ€í•œ ìì„¸í•œ ìš”ì•½ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”. ëª¨ë“  ì¤‘ìš”í•œ ì •ë³´, ë…¼ìŸ ë° ì—°ê²°ì„ ìœ ì§€í•©ë‹ˆë‹¤.',
        'pt': 'Crie um resumo detalhado da seÃ§Ã£o a seguir em portuguÃªs. Mantenha todas as informaÃ§Ãµes importantes, argumentos e conexÃµes.',
        'ar': 'Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ø®Øµ Ù…ÙØµÙ„ Ù„Ù„Ù‚Ø³Ù… Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø© ÙˆØ§Ù„Ø­Ø¬Ø¬ ÙˆØ§Ù„Ø§ØªØµØ§Ù„Ø§Øª.',
        'tr': 'AÅŸaÄŸÄ±daki bÃ¶lÃ¼mÃ¼n TÃ¼rkÃ§e ayrÄ±ntÄ±lÄ± bir Ã¶zetini oluÅŸturun. TÃ¼m Ã¶nemli bilgileri, argÃ¼manlarÄ± ve baÄŸlantÄ±larÄ± koruyun.',
        'bn': 'à¦¨à¦¿à¦®à§à¦¨à¦²à¦¿à¦–à¦¿à¦¤ à¦…à¦‚à¦¶à§‡à¦° à¦¬à¦¾à¦‚à¦²à¦¾à¦¯à¦¼ à¦à¦•à¦Ÿà¦¿ à¦¬à¦¿à¦¶à¦¦ à¦¸à¦‚à¦•à§à¦·à¦¿à¦ªà§à¦¤à¦¸à¦¾à¦° à¦¤à§ˆà¦°à¦¿ à¦•à¦°à§à¦¨à¥¤ à¦¸à¦®à¦¸à§à¦¤ à¦—à§à¦°à§à¦¤à§à¦¬à¦ªà§‚à¦°à§à¦£ à¦¤à¦¥à§à¦¯, à¦¯à§à¦•à§à¦¤à¦¿ à¦à¦¬à¦‚ à¦¸à¦‚à¦¯à§‹à¦—à¦—à§à¦²à¦¿ à¦¬à¦œà¦¾à¦¯à¦¼ à¦°à¦¾à¦–à§à¦¨à¥¤',
        'mr': 'à¤–à¤¾à¤²à¥€à¤² à¤µà¤¿à¤­à¤¾à¤—à¤¾à¤šà¤¾ à¤®à¤°à¤¾à¤ à¥€à¤¤ à¤¸à¤µà¤¿à¤¸à¥à¤¤à¤° à¤†à¤¢à¤¾à¤µà¤¾ à¤˜à¥à¤¯à¤¾. à¤¸à¤°à¥à¤µ à¤®à¤¹à¤¤à¥à¤¤à¥à¤µà¤¾à¤šà¥€ à¤®à¤¾à¤¹à¤¿à¤¤à¥€, à¤¤à¤°à¥à¤• à¤†à¤£à¤¿ à¤•à¤¨à¥‡à¤•à¥à¤¶à¤¨ à¤•à¤¾à¤¯à¤® à¤ à¥‡à¤µà¤¾.',
        'ta': 'à®•à¯€à®´à¯à®•à¯à®•à®£à¯à®Ÿ à®ªà®•à¯à®¤à®¿à®¯à®¿à®©à¯ à®¤à®®à®¿à®´à®¿à®²à¯ à®µà®¿à®°à®¿à®µà®¾à®© à®šà¯à®°à¯à®•à¯à®•à®¤à¯à®¤à¯ˆ à®‰à®°à¯à®µà®¾à®•à¯à®•à¯à®™à¯à®•à®³à¯. à®…à®©à¯ˆà®¤à¯à®¤à¯ à®®à¯à®•à¯à®•à®¿à®¯à®®à®¾à®© à®¤à®•à®µà®²à¯à®•à®³à¯ˆà®¯à¯à®®à¯ à®µà®¾à®¤à®™à¯à®•à®³à¯ˆà®¯à¯à®®à¯ à®‡à®£à¯ˆà®ªà¯à®ªà¯à®•à®³à¯ˆà®¯à¯à®®à¯ à®ªà®°à®¾à®®à®°à®¿à®•à¯à®•à®µà¯à®®à¯.',
        'te': 'à°•à±à°°à°¿à°‚à°¦à°¿ à°µà°¿à°­à°¾à°—à°‚ à°¯à±Šà°•à±à°• à°¤à±†à°²à±à°—à± à°²à±‹ à°µà°¿à°µà°°à°®à±ˆà°¨ à°¸à°¾à°°à°¾à°‚à°¶à°¾à°¨à±à°¨à°¿ à°¸à±ƒà°·à±à°Ÿà°¿à°‚à°šà°‚à°¡à°¿. à°…à°¨à±à°¨à°¿ à°®à±à°–à±à°¯à°®à±ˆà°¨ à°¸à°®à°¾à°šà°¾à°°à°¾à°¨à±à°¨à°¿, à°µà°¾à°¦à°¨à°²à°¨à± à°®à°°à°¿à°¯à± à°¸à°‚à°¬à°‚à°§à°¾à°²à°¨à± à°¨à°¿à°°à±à°µà°¹à°¿à°‚à°šà°‚à°¡à°¿.',
        'kn': 'à²•à³†à²³à²—à²¿à²¨ à²µà²¿à²­à²¾à²—à²¦ à²•à²¨à³à²¨à²¡à²¦à²²à³à²²à²¿ à²µà²¿à²µà²°à²µà²¾à²¦ à²¸à²‚à²•à³à²·à²¿à²ªà³à²¤ à²µà²¿à²µà²°à²µà²¨à³à²¨à³ à²°à²šà²¿à²¸à²¿. à²à²²à³à²²à²¾ à²®à³à²–à³à²¯ à²®à²¾à²¹à²¿à²¤à²¿à²¯à²¨à³à²¨à³, à²µà²¾à²¦à²—à²³à²¨à³à²¨à³ à²®à²¤à³à²¤à³ à²¸à²‚à²ªà²°à³à²•à²—à²³à²¨à³à²¨à³ à²‰à²³à²¿à²¸à²¿.',
        'ml': 'à´¤à´¾à´´àµ† à´ªà´±à´¯àµà´¨àµà´¨ à´µà´¿à´­à´¾à´—à´¤àµà´¤à´¿à´¨àµà´±àµ† à´®à´²à´¯à´¾à´³à´¤àµà´¤à´¿àµ½ à´µà´¿à´¶à´¦à´®à´¾à´¯ à´¸à´¾à´°à´¾à´‚à´¶à´‚ à´¸àµƒà´·àµà´Ÿà´¿à´•àµà´•àµà´•. à´à´²àµà´²à´¾ à´ªàµà´°à´§à´¾à´¨ à´µà´¿à´µà´°à´™àµà´™à´³àµà´‚ à´µà´¾à´¦à´™àµà´™à´³àµà´‚ à´¬à´¨àµà´§à´™àµà´™à´³àµà´‚ à´¨à´¿à´²à´¨à´¿àµ¼à´¤àµà´¤àµà´•.',
        'bh': 'à¤¨à¤¿à¤®à¥à¤¨à¤²à¤¿à¤–à¤¿à¤¤ à¤–à¤‚à¤¡ à¤•à¤¾ à¤­à¥‹à¤œà¤ªà¥à¤°à¥€ à¤®à¥‡à¤‚ à¤à¤• à¤µà¤¿à¤¸à¥à¤¤à¥ƒà¤¤ à¤¸à¤¾à¤°à¤¾à¤‚à¤¶ à¤¬à¤¨à¤¾à¤ˆà¤‚à¥¤ à¤¸à¤­à¥‡ à¤®à¤¹à¤¤à¥à¤µà¤ªà¥‚à¤°à¥à¤£ à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€, à¤¤à¤°à¥à¤• à¤…à¤‰à¤° à¤¸à¤‚à¤¬à¤‚à¤§à¤¨ à¤•à¥‡ à¤¬à¤¨à¤¾à¤µà¥‡ à¤•à¥‡ à¤•à¤¾à¤® à¤•à¤°à¥€à¤‚à¥¤'
    }

    instruction = language_instructions.get(language_code, language_instructions['en'])

    if mode == 'podcast':
        instruction += ' Present this summary in a narrative, engaging style suitable for a podcast. Ensure a natural flow with storytelling elements.'
    elif mode == 'video':
        instruction += ' Present this summary in a structured, concise format suitable for a video. Use bullet points and headings to clearly organize the key points.'
    else:
        raise ValueError("Invalid mode. Choose either 'podcast' or 'video'.")

    system_prompt = f"""You are an expert content summarizer. {instruction}
    Ensure the summary is fully in {language_code}, without any words from other languages."""

    user_prompt = f"""{instruction}
    Pay special attention to:
    - Main topics and arguments
    - Important details and examples
    - Connections with other mentioned topics
    - Key statements and conclusions

    Text: {text_chunk}"""

    return system_prompt, user_prompt

# Function to summarize with Langchain and OpenAI
def summarize_with_langchain_and_openai(transcript, mode, language_code='en', model_name='llama-3.1-8b-instant'):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=7000,
        chunk_overlap=1000,
        length_function=len
    )
    texts = text_splitter.split_text(transcript)

    intermediate_summaries = []
    
    for i, text_chunk in enumerate(texts):
        system_prompt, user_prompt = create_summary_prompt(text_chunk, language_code, mode)
        
        try:
            response = groq_client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.7,  # Keeping temperature low to maintain consistency
                max_tokens=8000
            )
            
            summary = response.choices[0].message.content
            intermediate_summaries.append(summary)
            
        except Exception as e:
            st.error(f"Error with Groq API during intermediate summarization: {str(e)}")
            return None

    combined_summary = "\n\n=== Next Section ===\n\n".join(intermediate_summaries)
    
    final_instruction = 'Maintain a narrative and engaging style, making sure to connect the points naturally and conversationally. Use transitions and storytelling elements to keep it engaging.' if mode == 'podcast' else 'Keep the summary concise and well-structured, focusing on key points and details. Use bullet points and headings to organize the content clearly.'

    final_system_prompt = f"""You are an expert in creating comprehensive summaries. 
    Create a coherent, well-structured complete summary in {language_code} from the 
    provided intermediate summaries. Connect the information logically and establish 
    important relationships. Ensure the summary is fully in {language_code}, without any words from other languages.
    {final_instruction}"""
    
    final_user_prompt = f"""Create a final, comprehensive summary from the following 
    intermediate summaries. The summary should be fully in {language_code}, without any words from other languages.
    - Include all important topics and arguments
    - Establish logical connections between topics
    - Have a clear structure
    - Highlight key statements and most important insights
    {final_instruction}

    Intermediate summaries:
    {combined_summary}"""
    
    try:
        final_response = groq_client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": final_system_prompt},
                {"role": "user", "content": final_user_prompt}
            ],
            temperature=0.7,  # Keeping temperature low to maintain consistency
            max_tokens=8000
        )
        
        final_summary = final_response.choices[0].message.content
        return final_summary
    except Exception as e:
        st.error(f"Error with Groq API during final summarization: {str(e)}")
        return None


class PDF(FPDF):
    def header(self):
        self.set_font('FreeSerif', 'B', 12)
        self.cell(0, 10, 'Summary Report', 0, 1, 'C')
        self.ln(10)

    def chapter_title(self, title):
        self.set_font('FreeSerif', 'B', 12)
        self.cell(0, 10, title, 0, 1, 'L')
        self.ln(5)

    def chapter_body(self, body):
        self.set_font('FreeSerif', '', 12)
        for line in body.split('\n'):
            if line.strip().endswith(':'):
                self.set_font('FreeSerif', 'B', 12)
                self.multi_cell(0, 10, line.strip())
                self.set_font('FreeSerif', '', 12)
            elif line.startswith('* '):
                self.set_font('FreeSerif', '', 12)
                self.multi_cell(0, 10, u'\u2022 ' + line[2:])
            else:
                self.multi_cell(0, 10, line)
            self.ln(5)

# Function to generate PDF
def generate_pdf(summary, title="Summary"):
    pdf = PDF()

    # Load Unicode fonts from the specified path
    font_path = 'C:\\coding\\youtube summarizer\\youtube_summarizer\\textFormat\\freeserif\\'
    pdf.add_font('FreeSerif', '', font_path + 'FreeSerif.ttf', uni=True)
    pdf.add_font('FreeSerif', 'B', font_path + 'FreeSerifBold.ttf', uni=True)
    pdf.add_font('FreeSerif', 'I', font_path + 'FreeSerifItalic.ttf', uni=True)
    pdf.add_font('FreeSerif', 'BI', font_path + 'FreeSerifBoldItalic.ttf', uni=True)

    pdf.add_page()
    pdf.chapter_title(title)
    pdf.chapter_body(summary)

    # Output PDF to a string
    pdf_output = pdf.output(dest='S').encode('latin1')
    return pdf_output

# Function to generate DOCX
def generate_doc(summary, title="Summary"):
    doc = Document()
    doc.add_heading(title, 0)

    for line in summary.split('\n'):
        if line.startswith('**') and line.endswith('**'):
            paragraph = doc.add_paragraph()
            run = paragraph.add_run(line.strip('**'))
            run.bold = True
        elif line.strip().endswith(':'):
            doc.add_heading(line.strip(), level=2)
        elif line.startswith('* '):
            doc.add_paragraph(line[2:], style='List Bullet')
        else:
            doc.add_paragraph(line)

    with BytesIO() as doc_output:
        doc.save(doc_output)
        doc_output.seek(0)
        return doc_output.read()

# Function to analyze sentiment
def analyze_sentiment(text):
    analyzer = SentimentIntensityAnalyzer()
    sentiment = analyzer.polarity_scores(text)
    return sentiment

def map_key_points_to_intervals(key_points, total_duration):
    """
    Maps key points to evenly distributed time intervals across the video duration.

    Args:
        key_points (list): List of key points (strings).
        total_duration (int): Total duration of the video in seconds.

    Returns:
        list: List of tuples representing (start_time, end_time) for each key point.
    """
    num_points = len(key_points)
    interval_duration = total_duration / num_points  # Divide video into equal segments
    
    intervals = []
    for i in range(num_points):
        start_time = i * interval_duration
        end_time = min((i + 1) * interval_duration, total_duration)
        intervals.append((start_time, end_time))
    
    return intervals

def find_subtitle_index(current_time, intervals):
    """
    Finds the index of the subtitle that corresponds to the given time.

    Args:
        current_time (float): Current time in seconds.
        intervals (list): List of (start_time, end_time) intervals.

    Returns:
        int or None: Index of the subtitle if found, otherwise None.
    """
    for idx, (start_time, end_time) in enumerate(intervals):
        if start_time <= current_time < end_time:
            return idx
    return None

def extract_key_points(summary):
    # Assuming key points are indicated by bullet points or specific markers in the summary
    key_points = [line for line in summary.split('\n') if line.startswith('ğŸ”‘') or line.startswith('* ')]
    return key_points

def create_highlight_reels(video_path, key_points, subtitles, reel_duration=60):

    # Load video with OpenCV
    cap = cv2.VideoCapture(video_path)
    
    # Get video properties
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    total_duration = total_frames / fps  # in seconds
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    # Convert key points to time intervals
    intervals = map_key_points_to_intervals(key_points, total_duration)
    
    # Calculate number of reels
    num_reels = math.ceil(total_duration / reel_duration)
    reel_paths = []

    for i in range(num_reels):
        start_time = i * reel_duration
        end_time = min((i + 1) * reel_duration, total_duration)
        
        output_video_path = f'reel_{i + 1}.mp4'
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))
        
        cap.set(cv2.CAP_PROP_POS_MSEC, start_time * 1000)
        while cap.isOpened() and cap.get(cv2.CAP_PROP_POS_MSEC) < end_time * 1000:
            ret, frame = cap.read()
            if not ret:
                break
            
            current_time = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000
            subtitle_index = find_subtitle_index(current_time, intervals)

            if subtitle_index is not None:
                current_subtitle = subtitles[subtitle_index]
                cv2.putText(frame, current_subtitle, (10, height - 30), 
                            cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)
            
            out.write(frame)

        out.release()
        reel_paths.append(output_video_path)
    
    cap.release()
    return reel_paths

def get_time_interval(point):
    # Placeholder for actual mapping of points to time intervals
    time_intervals = {
        "First Key Point": (10, 20),  # start time in seconds, end time in seconds
        "Second Key Point": (30, 45),
        "Third Key Point": (50, 65)
        # Add more mappings as needed
    }
    return time_intervals.get(point.strip(), (0, 5))  # default to the first 5 seconds if not found

def download_youtube_video(url, cookies_path=None):
    try:
        ydl_opts = {
            'format': 'bestaudio+bvideo',  # Download best audio and video streams
            'outtmpl': 'videos/%(title)s.%(ext)s',  # Save video with title
            'quiet': False,  # Show progress during download
        }

        if cookies_path:
            ydl_opts['cookiefile'] = cookies_path

        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info_dict = ydl.extract_info(url, download=True)
            video_path = ydl.prepare_filename(info_dict)
            return video_path
    except Exception as e:
        print(f"Error downloading the video: {str(e)}")
        return None


def main():
    st.title('ğŸ“º Advanced YouTube Video Summarizer')
    st.markdown("""
    This tool creates comprehensive summaries of YouTube videos using advanced AI technology.
    It works with both videos that have transcripts and those that don't!
    """)

    # Initialize session state variables
    if 'link' not in st.session_state:
        st.session_state.link = ""
    if 'language' not in st.session_state:
        st.session_state.language = ""
    if 'mode' not in st.session_state:
        st.session_state.mode = ""
    if 'summary' not in st.session_state:
        st.session_state.summary = None
    if 'sentiment' not in st.session_state:
        st.session_state.sentiment = None
    if 'reel' not in st.session_state:
        st.session_state.reel = None

    col1, col2, col3 = st.columns([3, 1, 1])
    
    with col1:
        link = st.text_input('ğŸ”— Enter YouTube video URL:', key='link_input')
    
    with col2:
        languages = get_available_languages()
        target_language = st.selectbox(
            'ğŸŒ Select Summary Language:',
            options=list(languages.keys()),
            index=list(languages.keys()).index(st.session_state.language) if st.session_state.language else 0,
            key='language_input'
        )
        target_language_code = languages[target_language]

    with col3:
        mode = st.selectbox(
            'ğŸ™ï¸ Select Mode:',
            options=['video', 'podcast'],
            index=['video', 'podcast'].index(st.session_state.mode) if st.session_state.mode else 0,
            key='mode_input'
        )
        mode = mode.lower()

    # Check for changes in link, language, or mode
    if (link != st.session_state.link or 
        target_language != st.session_state.language or 
        mode != st.session_state.mode):
        st.session_state.link = link
        st.session_state.language = target_language
        st.session_state.mode = mode
        st.session_state.summary = None
        st.session_state.sentiment = None
        st.session_state.reel = None

    if st.button('Generate Summary'):
        if link:
            try:
                with st.spinner('Processing...'):
                    progress = st.progress(0)
                    status_text = st.empty()

                    status_text.text('ğŸ“¥ Fetching video transcript...')
                    progress.progress(25)

                    transcript, _ = get_transcript(link)

                    status_text.text(f'ğŸ¤– Generating {target_language} summary...')
                    progress.progress(75)

                    summary = summarize_with_langchain_and_openai(
                        transcript, 
                        mode,
                        target_language_code,
                        model_name='llama-3.1-8b-instant'
                    )
                    
                    sentiment = analyze_sentiment(transcript)

                    status_text.text('âœ¨ Summary Ready!')
                    # Save summary and sentiment in session state
                    st.session_state.summary = summary
                    st.session_state.sentiment = sentiment

                    progress.progress(100)

            except Exception as e:
                st.error(f"An error occurred: {str(e)}")
        else:
            st.warning('Please enter a valid YouTube link.')

    # Display summary, sentiment, and download buttons if summary exists
    if st.session_state.summary:
        st.markdown(st.session_state.summary)

        sentiment = st.session_state.sentiment
        if sentiment:
            st.markdown("### Sentiment Analysis")
            st.json(sentiment)
            neg = sentiment['neg'] * 100
            neu = sentiment['neu'] * 100
            pos = sentiment['pos'] * 100
            compound = sentiment['compound']
            st.markdown(f"""
            **Negative Sentiment**: {neg:.1f}% - Parts of the text have a negative tone, such as criticism, sadness, or anger.\n
            **Neutral Sentiment**: {neu:.1f}% - The majority of the text is neutral, without strong positive or negative emotions.\n
            **Positive Sentiment**: {pos:.1f}% - Parts of the text have a positive tone, expressing happiness, praise, or optimism.\n
            **Overall Sentiment**: {compound:.4f} - The overall tone of the text is { 'strongly negative' if compound < -0.5 else 'negative' if compound < 0 else 'neutral' if compound == 0 else 'positive' if compound > 0 else 'strongly positive' }.
            """)
            st.markdown(f"""
            **Overall Summary**:
            - **Mostly Neutral**: The majority of the content (about {neu:.1f}%) is neutral, indicating factual or indifferent statements.
            - **Negative Sentiment**: Approximately {neg:.1f}% of the content has a negative tone, suggesting some criticism, sadness, or anger.
            - **Positive Sentiment**: Around {pos:.1f}% of the content is positive, showing instances of happiness, praise, or optimism.
            - **Strong Overall Negative Tone**: The overall sentiment score of {compound:.4f} indicates that the general tone of the video is quite negative.
            """)

        key_points = extract_key_points(st.session_state.summary)
        subtitles = [point.strip('ğŸ”‘').strip('* ') for point in key_points]

        if st.button('Create Reel(s)'):
            with st.spinner('Creating reel(s)...'):
                video_path = download_youtube_video(link)
                if video_path:
                    subtitles = [point.strip('ğŸ”‘').strip('* ') for point in key_points]
                    reel_paths = create_highlight_reels(video_path, key_points, subtitles, reel_duration=60)
                    if reel_paths:
                        st.success(f'{len(reel_paths)} reel(s) created successfully!')
                        for i, reel_path in enumerate(reel_paths):
                            st.markdown(f"### Reel {i + 1}")
                            st.download_button(
                                label=f"Download Reel {i + 1} (MP4)",
                                data=open(reel_path, 'rb').read(),
                                file_name=f"reel_{i + 1}.mp4",
                                mime="video/mp4")
                    else:
                        st.error('Failed to create reels.')
                else:
                    st.error('Video download failed. Reel creation aborted.')

        if st.session_state.reel:
            st.markdown('### Download Reel')
            st.download_button(
                label="Download Reel (MP4)",
                data=open(st.session_state.reel, 'rb').read(),
                file_name="reel.mp4",
                mime="video/mp4"
            )

        pdf_data = generate_pdf(st.session_state.summary)
        if pdf_data:
            st.download_button(
                label="Download as PDF",
                data=pdf_data,
                file_name="summary.pdf",
                mime="application/pdf"
            )
        
        doc_data = generate_doc(st.session_state.summary)
        st.download_button(
            label="Download as DOCX",
                data=doc_data,
                file_name="summary.docx",
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        )

if __name__ == "__main__":
    main()
